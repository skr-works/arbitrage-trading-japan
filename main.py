from __future__ import annotations

import io
import json
import os
import re
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
import requests
from bs4 import BeautifulSoup
import yfinance as yf
import pdfplumber

STATE_PATH = Path("state.json")

# ====== ä»•æ§˜ï¼ˆã‚ãªãŸã®æ±ºå®šï¼‰ ======
MA_DAYS = 20
MAX_HISTORY_DAYS = 900  # 3å¹´ â‰’ 756å–¶æ¥­æ—¥ + ãƒãƒƒãƒ•ã‚¡

ARB_BUY_RATIO_TH = 1.5
PRIME_VOL_RATIO_TH = 0.85
SQ_NEAR_DAYS = 5

# æŒ‡æ•°é«˜å€¤åœï¼ˆéå»3å¹´ï¼‰åˆ¤å®š
INDEX_PCTL = 0.90  # 90%ç‚¹
INDEX_TICKER = os.getenv("INDEX_TICKER", "^N225")  # ãƒ‡ãƒ•ã‚©: æ—¥çµŒ225ã€‚TOPIXã«ã—ãŸã‘ã‚Œã° ^TOPX ç­‰ã‚’è¨­å®š
INDEX_LOOKBACK = "3y"

JPX_PROGRAM_URL = "https://www.jpx.co.jp/markets/statistics-equities/program/index.html"
JPX_DAILY_URL = "https://www.jpx.co.jp/markets/statistics-equities/daily/index.html"

UA = "Mozilla/5.0 (compatible; jpx-bot/1.0; +https://github.com/)"


def fmt_bool(x: bool) -> str:
    return "TRUE" if x else "FALSE"


def fmt_num(x) -> str:
    if x is None:
        return "N/A"
    try:
        return f"{float(x):,.4f}"
    except Exception:
        return str(x)


def pick_level(alert: bool, conds: Dict[str, bool]) -> Tuple[str, str]:
    """
    LEVELã®ãƒ«ãƒ¼ãƒ«ï¼ˆå›ºå®šï¼‰:
      - LEVEL 3: ALERT=True
      - LEVEL 2: ALERT=False ã‹ã¤ æ¡ä»¶ãŒ2ã¤ä»¥ä¸ŠTRUE
      - LEVEL 1: ãã‚Œä»¥å¤–
    """
    true_cnt = sum(1 for v in conds.values() if v)
    if alert:
        return (
            "LEVEL 3: WARNING (è­¦æˆ’)",
            "ã€è­¦æˆ’ã€‘æ€¥å¤‰ã—ã‚„ã™ã„æ¡ä»¶ãŒæƒã£ã¦ã„ã¾ã™ã€‚å»ºç‰ã‚µã‚¤ã‚ºãƒ»æ–°è¦æŠ•å…¥ã‚’æŠ‘ãˆã€SQé€±ã¯ç‰¹ã«æ…é‡ã«ã€‚",
        )
    if true_cnt >= 2:
        return (
            "LEVEL 2: CAUTION (æ³¨æ„)",
            "ã€æ³¨æ„ã€‘ä¸€éƒ¨ã®æ­ªã¿ãŒå‡ºã¦ã„ã¾ã™ã€‚ç„¡ç†ãªè²·ã„æ–¹ï¼ˆãƒ¬ãƒ/ä¸€æ‹¬ï¼‰ã‚’é¿ã‘ã€åˆ†å‰²ã¨ä½™åŠ›é‡è¦–ã€‚",
        )
    return ("LEVEL 1: NORMAL (æ­£å¸¸)", "ã€é †è¡Œã€‘æ§‹é€ çš„ãªå±æ©Ÿæ¡ä»¶ã¯æœªæˆç«‹ã€‚é€šå¸¸é‹ç”¨ã§å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚")


def print_report(latest: Dict):
    idx = latest["inputs"]["index"]
    conds = latest["conditions"]
    metrics = latest["metrics"]
    thr = latest["thresholds"]
    alert = latest["alert"]["volatility_risk"]

    level_title, headline = pick_level(alert, conds)

    print("#" * 60)
    print(f"   {level_title}")
    print("#" * 60)
    print("")
    print("[ç·åˆåˆ¤å®šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]")
    print(headline)
    print("")
    print("#" * 60)
    print("=" * 60)
    print("ğŸ“Š JPX è£å®šãƒ»SQãƒ»æµå‹•æ€§ãƒ¬ãƒãƒ¼ãƒˆ (v1.0)")
    print("=" * 60)
    print(f"AsOf: {latest['asof']}")
    print("")
    print("[å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜]")
    print(f"- è£å®šå–å¼•ï¼ˆJPXï¼‰: {latest['inputs']['arb_date']}  â€»JPXã¯é…å»¶ã®å¯èƒ½æ€§ã‚ã‚Š")
    print(f"- ãƒ—ãƒ©ã‚¤ãƒ å‡ºæ¥é«˜ï¼ˆJPXæ—¥å ±ï¼‰: {latest['inputs']['prime_volume_date']}")
    print(f"- æŒ‡æ•°ï¼ˆ{idx['ticker']}ï¼‰: {idx['index_latest_date']}ï¼ˆçµ‚å€¤ãƒ™ãƒ¼ã‚¹ï¼‰")
    print("-" * 60)
    print("")

    # 1) Arbitrage
    print("1. Condition: Arbitrage Stretchï¼ˆè£å®šè²·ã„æ®‹ã®ç©ã¿ä¸ŠãŒã‚Šï¼‰")
    print(
        f"   çµæœ: {fmt_num(metrics['arb_buy_ratio_ma20'])}  (é–¾å€¤: >= {thr['arb_buy_ratio_ma20_hot']}) â†’ [{fmt_bool(conds['arb_buy_hot'])}]"
    )
    print("   [åˆ†æ]:")
    if metrics["arb_buy_ratio_ma20"] is None:
        print("   - ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆMA20æœªæˆç«‹ï¼‰ã€‚20å–¶æ¥­æ—¥åˆ†ãŒè²¯ã¾ã‚‹ã¾ã§åˆ¤å®šä¿ç•™ã€‚")
    elif conds["arb_buy_hot"]:
        print("   - è£å®šè²·ã„æ®‹ãŒå¹³å¸¸ã‚ˆã‚Šå¤§ããã€è§£æ¶ˆãŒèµ°ã‚‹ã¨ç¾ç‰©å£²ã‚Šåœ§ãŒå‡ºã‚„ã™ã„çŠ¶æ…‹ã§ã™ã€‚")
    else:
        print("   - è£å®šè²·ã„æ®‹ã¯å¹³å¸¸ãƒ¬ãƒ³ã‚¸ã€‚éœ€çµ¦ã®â€œç«è–¬åº«â€ã¯å¤§ããã‚ã‚Šã¾ã›ã‚“ã€‚")
    print("")

    # 2) SQ near
    print("2. Trigger: SQ Nearï¼ˆSQæ¥è¿‘ï¼‰")
    print(
        f"   çµæœ: days_to_2nd_fri = {metrics['days_to_2nd_fri']}  (é–¾å€¤: <= {thr['sq_near_days']}) â†’ [{fmt_bool(conds['sq_near'])}]"
    )
    print("   [åˆ†æ]:")
    if metrics["days_to_2nd_fri"] is None:
        print("   - æ—¥ä»˜è¨ˆç®—ã«å¤±æ•—ï¼ˆæƒ³å®šå¤–ï¼‰ã€‚")
    elif conds["sq_near"]:
        print("   - ä¾¡æ ¼å·®ãŒç· ã¾ã‚Šã‚„ã™ã„æœŸé–“ã€‚è£å®šã®è§£æ¶ˆãŒåŒæ–¹å‘ã«å‡ºã‚‹ã¨å€¤ãŒé£›ã³ã‚„ã™ã„ã€‚")
    else:
        print("   - SQã¯è¿‘ãã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¤ãƒ™ãƒ³ãƒˆè¦å› ã¯å¼±ã„ã€‚")
    print("")

    # 3) Prime volume thin
    print("3. Trigger: Prime Liquidity Thinï¼ˆãƒ—ãƒ©ã‚¤ãƒ æµå‹•æ€§ã®è–„ã•ï¼‰")
    print(
        f"   çµæœ: {fmt_num(metrics['prime_volume_ratio_ma20'])}  (é–¾å€¤: <= {thr['prime_volume_ratio_ma20_thin']}) â†’ [{fmt_bool(conds['prime_volume_thin'])}]"
    )
    print("   [åˆ†æ]:")
    if metrics["prime_volume_ratio_ma20"] is None:
        print("   - ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆMA20æœªæˆç«‹ï¼‰ã€‚20å–¶æ¥­æ—¥åˆ†ãŒè²¯ã¾ã‚‹ã¾ã§åˆ¤å®šä¿ç•™ã€‚")
    elif conds["prime_volume_thin"]:
        print("   - å¸‚å ´ã®å—ã‘çš¿ãŒè–„ã„ã€‚å°ã•ãªè§£æ¶ˆã§ã‚‚å€¤ãŒæ»‘ã‚Šã‚„ã™ã„å±€é¢ã§ã™ã€‚")
    else:
        print("   - å‡ºæ¥é«˜ã¯å¹³å¸¸åŸŸã€‚å—ã‘çš¿ã¯æ¥µç«¯ã«è–„ãã‚ã‚Šã¾ã›ã‚“ã€‚")
    print("")

    # 4) Index high zone
    print(f"4. Condition: Index High Zoneï¼ˆæŒ‡æ•°ã®é«˜å€¤åœï¼šéå»3å¹´ p{int(thr['index_pctl']*100)}ï¼‰")
    print(
        f"   çµæœ: latest_close={fmt_num(idx['latest_close'])}, threshold={fmt_num(idx['threshold_close'])} â†’ [{fmt_bool(conds['index_high_zone'])}]"
    )
    print("   [åˆ†æ]:")
    if conds["index_high_zone"]:
        print("   - ä¾¡æ ¼ä½ç½®ãŒä¸Šå´ã«å¯„ã£ã¦ã„ã¾ã™ã€‚å´©ã‚Œã‚‹ã¨ãã®ä¸‹æ–¹å‘ã®æŒ¯ã‚ŒãŒå‡ºã‚„ã™ã„å´ã§ã™ã€‚")
    else:
        print("   - é«˜å€¤åœã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ä¾¡æ ¼ä½ç½®ã®â€œä¸Šè©°ã¾ã‚Šâ€è¦å› ã¯å¼±ã„ã€‚")
    print("-" * 60)

    print("")
    print("[æœ€çµ‚åˆ¤å®š]")
    print(f"ALERT_VOLATILITY_RISK = {fmt_bool(alert)}")
    print(f"Rule: {latest['alert']['rule']}")
    if alert:
        print("æˆç«‹æ¡ä»¶:")
        for k in latest["alert"]["reasons"]:
            print(f"- {k}")
    print("")


def sess() -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": UA})
    return s


def _abs_url(base: str, href: str) -> str:
    if href.startswith("http"):
        return href
    if href.startswith("/"):
        return "https://www.jpx.co.jp" + href
    base_dir = base.rsplit("/", 1)[0] + "/"
    return base_dir + href


def load_state() -> Dict:
    if STATE_PATH.exists():
        return json.loads(STATE_PATH.read_text(encoding="utf-8"))
    return {
        "meta": {"created_at": datetime.now().isoformat(), "updated_at": None, "version": 2},
        "history": [],
        "latest": {},
    }


def save_state(state: Dict) -> None:
    state["meta"]["updated_at"] = datetime.now().isoformat()
    STATE_PATH.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")


def upsert_history(state: Dict, record: Dict) -> None:
    ds = record["date"]
    hist = state["history"]

    for i, r in enumerate(hist):
        if r.get("date") == ds:
            merged = dict(r)
            for k, v in record.items():
                if k == "signals":
                    merged.setdefault("signals", {})
                    merged["signals"].update(v or {})
                else:
                    if v is not None:
                        merged[k] = v
            hist[i] = merged
            break
    else:
        hist.append(record)

    hist.sort(key=lambda x: x.get("date"))
    if len(hist) > MAX_HISTORY_DAYS:
        state["history"] = hist[-MAX_HISTORY_DAYS:]


def ma_ratio(values: List[float], window: int = MA_DAYS) -> Optional[float]:
    if len(values) < window:
        return None
    s = pd.Series(values, dtype="float64")
    ma = float(s.tail(window).mean())
    if ma == 0:
        return None
    return float(s.iloc[-1] / ma)


def days_to_2nd_friday(today: date) -> int:
    y, m = today.year, today.month
    first = date(y, m, 1)
    days_to_fri = (4 - first.weekday()) % 7  # 4=Fri
    first_fri = first + timedelta(days=days_to_fri)
    second_fri = first_fri + timedelta(days=7)
    return (second_fri - today).days


def is_sq_near(today: date) -> Tuple[bool, int]:
    d = days_to_2nd_friday(today)
    return (0 <= d <= SQ_NEAR_DAYS), d


# ========= JPX è£å®šå–å¼• (PDF Ver.) =========
def fetch_latest_arbitrage_url(s: requests.Session) -> Tuple[date, str]:
    """
    JPXã®ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°ã®ã€Œè£å®šå–å¼•ã€PDFã®URLã‚’å–å¾—ã™ã‚‹ã€‚
    """
    r = s.get(JPX_PROGRAM_URL, timeout=30)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "lxml")

    candidates = []

    # Method 1: Row Scanning (Text based)
    for tr in soup.find_all("tr"):
        text = tr.get_text(" ", strip=True)
        text = re.sub(r"\s+", " ", text)
        
        # Pattern A: 2026å¹´1æœˆ16æ—¥
        m = re.search(r"(\d{4})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥", text)
        if not m:
            m = re.search(r"(\d{4})/\s*(\d{1,2})/\s*(\d{1,2})", text)
            
        if m:
            y, mo, d = map(int, m.groups())
            dt = date(y, mo, d)
            link = tr.find("a", href=re.compile(r"\.pdf", re.IGNORECASE))
            if link:
                url = _abs_url(JPX_PROGRAM_URL, link["href"])
                candidates.append((dt, url))
                continue

    # Method 2: Filename Scanning (Fallback)
    if not candidates:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if not re.search(r"\.pdf", href, re.IGNORECASE):
                continue
            
            url = _abs_url(JPX_PROGRAM_URL, href)
            filename = href.split("/")[-1]
            
            # Pattern C: 20260116.pdf
            m8 = re.search(r"(20\d{2})(\d{2})(\d{2})", filename)
            if m8:
                y, mo, d = map(int, m8.groups())
                candidates.append((date(y, mo, d), url))
                continue
            
            # Pattern D: 260116.pdf (YYMMDD)
            m6 = re.search(r"(\d{2})(\d{2})(\d{2})", filename)
            if m6:
                y_short, mo, d = map(int, m6.groups())
                if 1 <= mo <= 12 and 1 <= d <= 31:
                    y = 2000 + y_short
                    candidates.append((date(y, mo, d), url))
                    continue

    if not candidates:
        raise RuntimeError("JPX program page: No arbitrage PDF links found.")

    candidates.sort(key=lambda x: x[0], reverse=True)
    return candidates[0]


def download_bytes(s: requests.Session, url: str) -> bytes:
    r = s.get(url, timeout=60)
    r.raise_for_status()
    return r.content


def extract_arbitrage_data_from_pdf(pdf_bytes: bytes) -> Tuple[float, float]:
    """
    è£å®šå–å¼•PDFã‹ã‚‰ã€Œè£å®šå£²ã‚Šæ®‹ã€ã€Œè£å®šè²·ã„æ®‹ã€ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: ã€Œ2. è£å®šå–å¼•ã«ä¿‚ã‚‹ç¾ç‰©ãƒã‚¸ã‚·ãƒ§ãƒ³ã€ã®è¡¨ã«ã‚ã‚‹ã€Œæ ªæ•°ã€è¡Œ
    """
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        # å…¨ãƒšãƒ¼ã‚¸ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ½å‡ºã—ã¦æ¢ç´¢
        for page in pdf.pages:
            tables = page.extract_tables()
            for table in tables:
                # ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã®å„è¡Œã‚’ãƒã‚§ãƒƒã‚¯
                for row in table:
                    # Noneã‚’é™¤å»ã—ã¦æ–‡å­—åˆ—åŒ–
                    clean_row = [str(x).replace(",", "").replace(" ", "").strip() if x else "" for x in row]
                    
                    # 1åˆ—ç›®ãŒã€Œæ ªæ•°ã€ã¾ãŸã¯ãã‚Œã«é¡ã™ã‚‹è¡Œã‚’æ¢ã™
                    if len(clean_row) > 0 and "æ ªæ•°" in clean_row[0]:
                        # æ•°å€¤ãŒå«ã¾ã‚Œã‚‹è¦ç´ ã‚’æŠ½å‡º
                        nums = []
                        for cell in clean_row:
                            # æ•°å­—ã®ã¿æŠ½å‡ºï¼ˆãƒã‚¤ãƒŠã‚¹ç­‰ã¯è€ƒæ…®ã—ãªã„ã€æ®‹é«˜ã¯æ­£æ•°å‰æï¼‰
                            # ãŸã ã—OCRç­‰ã§æ”¹è¡ŒãŒå…¥ã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ "123\n456" ç­‰ã‚‚ã‚±ã‚¢
                            # ã“ã“ã§ã¯å˜ç´”ã« "1048783" ã®ã‚ˆã†ãªæ•°å€¤ã‚’æ¢ã™
                            ms = re.findall(r"\d+", cell)
                            # é€£çµã—ã¦æ•°å€¤åŒ–ï¼ˆä¾‹: "1,048, 783" -> ["1048", "783"] -> 1048783ï¼‰
                            if ms:
                                val_str = "".join(ms)
                                try:
                                    nums.append(float(val_str))
                                except:
                                    pass
                        
                        # æŠ½å‡ºã§ããŸæ•°å€¤ã®æ•°ã§åˆ¤å®š
                        # é€šå¸¸ã®ä¸¦ã³: [æ ªæ•°, å£²ã‚Šå½“é™, å£²ã‚Šåˆè¨ˆ, å£²ã‚Šç¿Œé™, è²·ã„å½“é™, è²·ã„ç¿Œé™, è²·ã„åˆè¨ˆ]
                        # åˆè¨ˆåˆ—ã¯é€šå¸¸ã€Œå£²ã‚Šåˆè¨ˆã€ã€Œè²·ã„åˆè¨ˆã€ã®2ã¤ãŒå¿…è¦ã€‚
                        # æ•°å€¤ãƒªã‚¹ãƒˆã®ä¸­ã§ã€å£²ã‚Šæ®‹ã¨è²·ã„æ®‹ã‚’ç‰¹å®šã™ã‚‹ã€‚
                        # çµŒé¨“å‰‡: 
                        #   æ•°å€¤ãŒ2ã¤ã—ã‹ãªã„ -> [å£²ã‚Šåˆè¨ˆ, è²·ã„åˆè¨ˆ] (ç°¡æ˜“è¡¨)
                        #   æ•°å€¤ãŒ5ã¤ä»¥ä¸Šã‚ã‚‹ -> è©³ç´°è¡¨ã€‚
                        #     å£²ã‚Šåˆè¨ˆ = å‰åŠã®å¤§ããªå€¤ (ã‚ã‚‹ã„ã¯2ç•ªç›®)
                        #     è²·ã„åˆè¨ˆ = æœ€å¾Œã®å€¤ (ã‚ã‚‹ã„ã¯æœ€å¾Œã‹ã‚‰2ç•ªç›®)
                        
                        if len(nums) >= 2:
                            # PDFã®æ§‹é€ ä¸Šã€å³ç«¯ãŒã€Œè²·ã„åˆè¨ˆã€ã€ãã®å°‘ã—å·¦ãŒã€Œå£²ã‚Šåˆè¨ˆã€ã§ã‚ã‚‹ã“ã¨ãŒå¤šã„
                            # å…·ä½“çš„ã«ã¯:
                            #   Row: [æ ªæ•°, Sell_Near, Sell_Total, Sell_Far, Buy_Near, Buy_Far, Buy_Total]
                            #   nums: [Sell_Near, Sell_Total, Sell_Far, Buy_Near, Buy_Far, Buy_Total] (0ã‚’é™¤ã)
                            #   å£²ã‚Šåˆè¨ˆ: nums[1] (Sell_Total)
                            #   è²·ã„åˆè¨ˆ: nums[-1] (Buy_Total)
                            
                            # ãŸã ã—ã€Sell_Farãªã©ãŒ0ã§çœç•¥ã•ã‚ŒãŸã‚Šçµåˆã•ã‚ŒãŸã‚Šã™ã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€
                            # ã‚ˆã‚Šå®‰å…¨ãªç­–ã¨ã—ã¦ã€Œæœ€å¾Œã€ã‚’è²·ã„æ®‹ã€ã€ŒçœŸã‚“ä¸­ã‚ãŸã‚Šã€ã‚’å£²ã‚Šæ®‹ã¨æ¨å®šã™ã‚‹ãŒã€
                            # ã“ã“ã§ã¯PDFã®å…¸å‹çš„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆæä¾›ã•ã‚ŒãŸPDFï¼‰ã«å¾“ã†ã€‚
                            
                            arb_buy = nums[-1]  # æœ€å¾Œã®æ•°å€¤ãŒè²·ã„æ®‹åˆè¨ˆ
                            
                            # å£²ã‚Šæ®‹ã¯ã€æ•°å€¤ãŒå¤šã‘ã‚Œã°2ç•ªç›®ã€å°‘ãªã‘ã‚Œã°1ç•ªç›®
                            if len(nums) >= 5:
                                arb_sell = nums[1]
                            else:
                                arb_sell = nums[0]
                                
                            return arb_buy, arb_sell

    raise RuntimeError("Arbitrage data (Buy/Sell positions) not found in PDF tables.")


# ========= JPX æ—¥å ±ï¼ˆãƒ—ãƒ©ã‚¤ãƒ å£²è²·é«˜ï¼‰(Robust Ver. 3) =========
def fetch_latest_daily_pdf_url(s: requests.Session) -> Tuple[date, str]:
    r = s.get(JPX_DAILY_URL, timeout=30)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "lxml")

    candidates = []

    # Method 1: Row Scanning
    for tr in soup.find_all("tr"):
        text = tr.get_text(" ", strip=True)
        text = re.sub(r"\s+", " ", text)
        
        m = re.search(r"(\d{4})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥", text)
        if not m:
            m = re.search(r"(\d{4})/\s*(\d{1,2})/\s*(\d{1,2})", text)
        
        if m:
            y, mo, d = map(int, m.groups())
            dt = date(y, mo, d)
            link = tr.find("a", href=re.compile(r"\.pdf", re.IGNORECASE))
            if link:
                url = _abs_url(JPX_DAILY_URL, link["href"])
                candidates.append((dt, url))
                continue

    # Method 2: Filename Scanning
    if not candidates:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if not re.search(r"\.pdf", href, re.IGNORECASE):
                continue
            
            url = _abs_url(JPX_DAILY_URL, href)
            filename = href.split("/")[-1]
            
            # Pattern C: 20260116.pdf
            m8 = re.search(r"(20\d{2})(\d{2})(\d{2})", filename)
            if m8:
                y, mo, d = map(int, m8.groups())
                candidates.append((date(y, mo, d), url))
                continue

            # Pattern D: 260116.pdf (YYMMDD)
            m6 = re.search(r"(\d{2})(\d{2})(\d{2})", filename)
            if m6:
                y_short, mo, d = map(int, m6.groups())
                if 1 <= mo <= 12 and 1 <= d <= 31:
                    y = 2000 + y_short
                    candidates.append((date(y, mo, d), url))
                    continue

    if not candidates:
        raise RuntimeError("JPX daily report page: No PDF links found (all methods failed).")

    candidates.sort(key=lambda x: x[0], reverse=True)
    return candidates[0]


def extract_prime_volume_from_pdf(pdf_bytes: bytes) -> float:
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        lines: List[str] = []
        for page in pdf.pages:
            t = page.extract_text() or ""
            for ln in t.splitlines():
                ln = ln.strip()
                if ln:
                    lines.append(ln)

    keys = ["ãƒ—ãƒ©ã‚¤ãƒ ", "Prime", "æ±è¨¼ãƒ—ãƒ©ã‚¤ãƒ "]
    for ln in lines:
        if any(k in ln for k in keys):
            nums = re.findall(r"\d{1,3}(?:,\d{3})+(?:\.\d+)?|\d+(?:\.\d+)?", ln)
            if not nums:
                continue
            vals = []
            for s in nums:
                try:
                    vals.append(float(s.replace(",", "")))
                except Exception:
                    pass
            if vals:
                return max(vals)

    raise RuntimeError("Prime volume not found in daily PDF text")


# ========= æŒ‡æ•°ï¼ˆéå»3å¹´ã®é«˜å€¤åœï¼‰ =========
def fetch_index_high_zone(ticker: str, pctl: float, lookback: str) -> Dict:
    """
    éå»3å¹´ã®çµ‚å€¤åˆ†å¸ƒã«å¯¾ã—ã¦ã€æœ€æ–°çµ‚å€¤ãŒpctlä»¥ä¸Šã‹ã‚’åˆ¤å®šã€‚
    """
    df = yf.download(ticker, period=lookback, interval="1d", auto_adjust=False, progress=False)
    if df is None or df.empty:
        raise RuntimeError(f"yfinance: no data for ticker={ticker}")

    if "Close" not in df.columns:
        raise RuntimeError(f"yfinance: Close not found for ticker={ticker}")

    close = df["Close"].dropna()
    if close.empty:
        raise RuntimeError(f"yfinance: Close empty for ticker={ticker}")

    latest_close = float(close.iloc[-1])
    q = float(close.quantile(pctl))
    high_zone = latest_close >= q

    return {
        "ticker": ticker,
        "lookback": lookback,
        "pctl": pctl,
        "latest_close": latest_close,
        "threshold_close": q,
        "index_high_zone": bool(high_zone),
        "index_latest_date": close.index[-1].date().isoformat(),
    }


def compute_latest(state: Dict, index_info: Dict) -> Dict:
    hist = state["history"]

    arb_days = [r for r in hist if isinstance(r.get("arb_buy"), (int, float))]
    vol_days = [r for r in hist if isinstance(r.get("prime_volume"), (int, float))]

    latest = {
        "asof": datetime.now().astimezone().isoformat(),
        "inputs": {
            "arb_date": None,
            "arb_buy": None,
            "arb_sell": None,
            "prime_volume_date": None,
            "prime_volume": None,
            "index": index_info,
        },
        "metrics": {
            "arb_buy_ratio_ma20": None,
            "prime_volume_ratio_ma20": None,
            "days_to_2nd_fri": None,
            "index_latest_close": index_info["latest_close"],
            "index_threshold_close_pctl": index_info["threshold_close"],
        },
        "thresholds": {
            "arb_buy_ratio_ma20_hot": ARB_BUY_RATIO_TH,
            "prime_volume_ratio_ma20_thin": PRIME_VOL_RATIO_TH,
            "sq_near_days": SQ_NEAR_DAYS,
            "index_pctl": INDEX_PCTL,
        },
        "conditions": {
            "arb_buy_hot": False,
            "sq_near": False,
            "prime_volume_thin": False,
            "index_high_zone": bool(index_info["index_high_zone"]),
        },
        "alert": {
            "volatility_risk": False,
            "rule": "arb_buy_hot & sq_near & prime_volume_thin & index_high_zone",
            "reasons": [],
        },
    }

    # --- arbitrage condition ---
    if arb_days:
        arb_days.sort(key=lambda x: x["date"])
        series = [float(r["arb_buy"]) for r in arb_days]
        ratio = ma_ratio(series, MA_DAYS)
        arb_latest = arb_days[-1]
        today = date.fromisoformat(arb_latest["date"])
        sq_near, d2f = is_sq_near(today)

        latest["inputs"]["arb_date"] = arb_latest["date"]
        latest["inputs"]["arb_buy"] = float(arb_latest["arb_buy"])
        latest["inputs"]["arb_sell"] = float(arb_latest["arb_sell"])

        latest["metrics"]["arb_buy_ratio_ma20"] = ratio
        latest["metrics"]["days_to_2nd_fri"] = d2f

        arb_hot = ratio is not None and ratio >= ARB_BUY_RATIO_TH
        latest["conditions"]["arb_buy_hot"] = bool(arb_hot)
        latest["conditions"]["sq_near"] = bool(sq_near)

    # --- volume condition ---
    if vol_days:
        vol_days.sort(key=lambda x: x["date"])
        series = [float(r["prime_volume"]) for r in vol_days]
        ratio = ma_ratio(series, MA_DAYS)
        vol_latest = vol_days[-1]

        latest["inputs"]["prime_volume_date"] = vol_latest["date"]
        latest["inputs"]["prime_volume"] = float(vol_latest["prime_volume"])

        latest["metrics"]["prime_volume_ratio_ma20"] = ratio

        vol_thin = ratio is not None and ratio <= PRIME_VOL_RATIO_TH
        latest["conditions"]["prime_volume_thin"] = bool(vol_thin)

    # --- alert ---
    c = latest["conditions"]
    alert = c["arb_buy_hot"] and c["sq_near"] and c["prime_volume_thin"] and c["index_high_zone"]
    latest["alert"]["volatility_risk"] = bool(alert)
    latest["alert"]["reasons"] = [k for k, v in c.items() if v]

    state["latest"] = latest
    return latest


def main() -> None:
    s = sess()
    state = load_state()

    # 1) è£å®šæ®‹ï¼ˆæœ€æ–°åˆ†ï¼‰ PDFç‰ˆ
    arb_dt, arb_url = fetch_latest_arbitrage_url(s)
    arb_pdf = download_bytes(s, arb_url)
    arb_buy, arb_sell = extract_arbitrage_data_from_pdf(arb_pdf)

    upsert_history(
        state,
        {
            "date": arb_dt.isoformat(),
            "arb_buy": arb_buy,
            "arb_sell": arb_sell,
            "arb_net": arb_buy - arb_sell,
            "prime_volume": None,
            "signals": {},
            "src": {"arb_pdf": arb_url},
        },
    )

    # 2) æ—¥å ±ï¼ˆæœ€æ–°åˆ†ï¼‰ãƒ—ãƒ©ã‚¤ãƒ å£²è²·é«˜
    vol_dt, pdf_url = fetch_latest_daily_pdf_url(s)
    pdf_bytes = download_bytes(s, pdf_url)
    prime_volume = extract_prime_volume_from_pdf(pdf_bytes)

    upsert_history(
        state,
        {
            "date": vol_dt.isoformat(),
            "arb_buy": None,
            "arb_sell": None,
            "arb_net": None,
            "prime_volume": prime_volume,
            "signals": {},
            "src": {"daily_pdf": pdf_url},
        },
    )

    # 3) æŒ‡æ•°é«˜å€¤åœ
    index_info = fetch_index_high_zone(INDEX_TICKER, INDEX_PCTL, INDEX_LOOKBACK)

    # 4) åˆ¤å®šã¾ã¨ã‚ï¼ˆlatestï¼‰
    latest = compute_latest(state, index_info)
    save_state(state)

    # 5) runãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ¬ãƒãƒ¼ãƒˆå½¢å¼ï¼‰
    print_report(latest)


if __name__ == "__main__":
    main()
