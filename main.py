from __future__ import annotations

import io
import json
import os
import re
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
import requests
from bs4 import BeautifulSoup
import yfinance as yf
import pdfplumber

STATE_PATH = Path("state.json")

# ====== ä»•æ§˜ï¼ˆã‚ãªãŸã®æ±ºå®šï¼‰ ======
MA_DAYS = 20
MAX_HISTORY_DAYS = 900  # 3å¹´ â‰’ 756å–¶æ¥­æ—¥ + ãƒãƒƒãƒ•ã‚¡

ARB_BUY_RATIO_TH = 1.5
PRIME_VOL_RATIO_TH = 0.85
SQ_NEAR_DAYS = 5

# æŒ‡æ•°é«˜å€¤åœï¼ˆéå»3å¹´ï¼‰åˆ¤å®š
INDEX_PCTL = 0.90  # 90%ç‚¹
INDEX_TICKER = os.getenv("INDEX_TICKER", "^N225")  # ãƒ‡ãƒ•ã‚©: æ—¥çµŒ225
INDEX_LOOKBACK = "3y"

JPX_PROGRAM_URL = "https://www.jpx.co.jp/markets/statistics-equities/program/index.html"
JPX_DAILY_URL = "https://www.jpx.co.jp/markets/statistics-equities/daily/index.html"

UA = "Mozilla/5.0 (compatible; jpx-bot/1.0; +https://github.com/)"


def fmt_bool(x: bool) -> str:
    return "TRUE" if x else "FALSE"


def fmt_num(x) -> str:
    if x is None:
        return "N/A"
    try:
        return f"{float(x):,.4f}"
    except Exception:
        return str(x)


def pick_level(alert: bool, conds: Dict[str, bool]) -> Tuple[str, str]:
    """
    LEVELã®ãƒ«ãƒ¼ãƒ«:
      - LEVEL 3: ALERT=True
      - LEVEL 2: ALERT=False ã‹ã¤ æ¡ä»¶ãŒ2ã¤ä»¥ä¸ŠTRUE
      - LEVEL 1: ãã‚Œä»¥å¤–
    """
    true_cnt = sum(1 for v in conds.values() if v)
    if alert:
        return (
            "LEVEL 3: WARNING (è­¦æˆ’)",
            "ã€è­¦æˆ’ã€‘æ€¥å¤‰ã—ã‚„ã™ã„æ¡ä»¶ãŒæƒã£ã¦ã„ã¾ã™ã€‚å»ºç‰ã‚µã‚¤ã‚ºãƒ»æ–°è¦æŠ•å…¥ã‚’æŠ‘ãˆã€SQé€±ã¯ç‰¹ã«æ…é‡ã«ã€‚",
        )
    if true_cnt >= 2:
        return (
            "LEVEL 2: CAUTION (æ³¨æ„)",
            "ã€æ³¨æ„ã€‘ä¸€éƒ¨ã®æ­ªã¿ãŒå‡ºã¦ã„ã¾ã™ã€‚ç„¡ç†ãªè²·ã„æ–¹ï¼ˆãƒ¬ãƒ/ä¸€æ‹¬ï¼‰ã‚’é¿ã‘ã€åˆ†å‰²ã¨ä½™åŠ›é‡è¦–ã€‚",
        )
    return ("LEVEL 1: NORMAL (æ­£å¸¸)", "ã€é †è¡Œã€‘æ§‹é€ çš„ãªå±æ©Ÿæ¡ä»¶ã¯æœªæˆç«‹ã€‚é€šå¸¸é‹ç”¨ã§å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚")


def print_report(latest: Dict):
    idx = latest["inputs"]["index"]
    conds = latest["conditions"]
    metrics = latest["metrics"]
    thr = latest["thresholds"]
    alert = latest["alert"]["volatility_risk"]

    level_title, headline = pick_level(alert, conds)

    print("#" * 60)
    print(f"   {level_title}")
    print("#" * 60)
    print("")
    print("[ç·åˆåˆ¤å®šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]")
    print(headline)
    print("")
    print("#" * 60)
    print("=" * 60)
    print("ğŸ“Š JPX è£å®šãƒ»SQãƒ»æµå‹•æ€§ãƒ¬ãƒãƒ¼ãƒˆ (v1.0)")
    print("=" * 60)
    print(f"AsOf: {latest['asof']}")
    print("")
    print("[å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜]")
    print(f"- è£å®šå–å¼•ï¼ˆJPXï¼‰: {latest['inputs']['arb_date']}  â€»JPXã¯é…å»¶ã®å¯èƒ½æ€§ã‚ã‚Š")
    print(f"- ãƒ—ãƒ©ã‚¤ãƒ å‡ºæ¥é«˜ï¼ˆJPXæ—¥å ±ï¼‰: {latest['inputs']['prime_volume_date']}")
    print(f"- æŒ‡æ•°ï¼ˆ{idx['ticker']}ï¼‰: {idx['index_latest_date']}ï¼ˆçµ‚å€¤ãƒ™ãƒ¼ã‚¹ï¼‰")
    print("-" * 60)
    print("")

    # 1) Arbitrage
    print("1. Condition: Arbitrage Stretchï¼ˆè£å®šè²·ã„æ®‹ã®ç©ã¿ä¸ŠãŒã‚Šï¼‰")
    print(
        f"   çµæœ: {fmt_num(metrics['arb_buy_ratio_ma20'])}  (é–¾å€¤: >= {thr['arb_buy_ratio_ma20_hot']}) â†’ [{fmt_bool(conds['arb_buy_hot'])}]"
    )
    print("   [åˆ†æ]:")
    if metrics["arb_buy_ratio_ma20"] is None:
        print("   - ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆMA20æœªæˆç«‹ï¼‰ã€‚20å–¶æ¥­æ—¥åˆ†ãŒè²¯ã¾ã‚‹ã¾ã§åˆ¤å®šä¿ç•™ã€‚")
    elif conds["arb_buy_hot"]:
        print("   - è£å®šè²·ã„æ®‹ãŒå¹³å¸¸ã‚ˆã‚Šå¤§ããã€è§£æ¶ˆãŒèµ°ã‚‹ã¨ç¾ç‰©å£²ã‚Šåœ§ãŒå‡ºã‚„ã™ã„çŠ¶æ…‹ã§ã™ã€‚")
    else:
        print("   - è£å®šè²·ã„æ®‹ã¯å¹³å¸¸ãƒ¬ãƒ³ã‚¸ã€‚éœ€çµ¦ã®â€œç«è–¬åº«â€ã¯å¤§ããã‚ã‚Šã¾ã›ã‚“ã€‚")
    print("")

    # 2) SQ near
    print("2. Trigger: SQ Nearï¼ˆSQæ¥è¿‘ï¼‰")
    print(
        f"   çµæœ: days_to_2nd_fri = {metrics['days_to_2nd_fri']}  (é–¾å€¤: <= {thr['sq_near_days']}) â†’ [{fmt_bool(conds['sq_near'])}]"
    )
    print("   [åˆ†æ]:")
    if metrics["days_to_2nd_fri"] is None:
        print("   - æ—¥ä»˜è¨ˆç®—ã«å¤±æ•—ï¼ˆæƒ³å®šå¤–ï¼‰ã€‚")
    elif conds["sq_near"]:
        print("   - ä¾¡æ ¼å·®ãŒç· ã¾ã‚Šã‚„ã™ã„æœŸé–“ã€‚è£å®šã®è§£æ¶ˆãŒåŒæ–¹å‘ã«å‡ºã‚‹ã¨å€¤ãŒé£›ã³ã‚„ã™ã„ã€‚")
    else:
        print("   - SQã¯è¿‘ãã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¤ãƒ™ãƒ³ãƒˆè¦å› ã¯å¼±ã„ã€‚")
    print("")

    # 3) Prime volume thin
    print("3. Trigger: Prime Liquidity Thinï¼ˆãƒ—ãƒ©ã‚¤ãƒ æµå‹•æ€§ã®è–„ã•ï¼‰")
    print(
        f"   çµæœ: {fmt_num(metrics['prime_volume_ratio_ma20'])}  (é–¾å€¤: <= {thr['prime_volume_ratio_ma20_thin']}) â†’ [{fmt_bool(conds['prime_volume_thin'])}]"
    )
    print("   [åˆ†æ]:")
    if metrics["prime_volume_ratio_ma20"] is None:
        print("   - ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆMA20æœªæˆç«‹ï¼‰ã€‚20å–¶æ¥­æ—¥åˆ†ãŒè²¯ã¾ã‚‹ã¾ã§åˆ¤å®šä¿ç•™ã€‚")
    elif conds["prime_volume_thin"]:
        print("   - å¸‚å ´ã®å—ã‘çš¿ãŒè–„ã„ã€‚å°ã•ãªè§£æ¶ˆã§ã‚‚å€¤ãŒæ»‘ã‚Šã‚„ã™ã„å±€é¢ã§ã™ã€‚")
    else:
        print("   - å‡ºæ¥é«˜ã¯å¹³å¸¸åŸŸã€‚å—ã‘çš¿ã¯æ¥µç«¯ã«è–„ãã‚ã‚Šã¾ã›ã‚“ã€‚")
    print("")

    # 4) Index high zone
    print(f"4. Condition: Index High Zoneï¼ˆæŒ‡æ•°ã®é«˜å€¤åœï¼šéå»3å¹´ p{int(thr['index_pctl']*100)}ï¼‰")
    print(
        f"   çµæœ: latest_close={fmt_num(idx['latest_close'])}, threshold={fmt_num(idx['threshold_close'])} â†’ [{fmt_bool(conds['index_high_zone'])}]"
    )
    print("   [åˆ†æ]:")
    if conds["index_high_zone"]:
        print("   - ä¾¡æ ¼ä½ç½®ãŒä¸Šå´ã«å¯„ã£ã¦ã„ã¾ã™ã€‚å´©ã‚Œã‚‹ã¨ãã®ä¸‹æ–¹å‘ã®æŒ¯ã‚ŒãŒå‡ºã‚„ã™ã„å´ã§ã™ã€‚")
    else:
        print("   - é«˜å€¤åœã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ä¾¡æ ¼ä½ç½®ã®â€œä¸Šè©°ã¾ã‚Šâ€è¦å› ã¯å¼±ã„ã€‚")
    print("-" * 60)

    print("")
    print("[æœ€çµ‚åˆ¤å®š]")
    print(f"ALERT_VOLATILITY_RISK = {fmt_bool(alert)}")
    print(f"Rule: {latest['alert']['rule']}")
    if alert:
        print("æˆç«‹æ¡ä»¶:")
        for k in latest["alert"]["reasons"]:
            print(f"- {k}")
    print("")


def sess() -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": UA})
    return s


def _abs_url(base: str, href: str) -> str:
    if href.startswith("http"):
        return href
    if href.startswith("/"):
        return "https://www.jpx.co.jp" + href
    base_dir = base.rsplit("/", 1)[0] + "/"
    return base_dir + href


def load_state() -> Dict:
    if STATE_PATH.exists():
        return json.loads(STATE_PATH.read_text(encoding="utf-8"))
    return {
        "meta": {"created_at": datetime.now().isoformat(), "updated_at": None, "version": 2},
        "history": [],
        "latest": {},
    }


def save_state(state: Dict) -> None:
    state["meta"]["updated_at"] = datetime.now().isoformat()
    STATE_PATH.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")


def upsert_history(state: Dict, record: Dict) -> None:
    ds = record["date"]
    hist = state["history"]

    for i, r in enumerate(hist):
        if r.get("date") == ds:
            merged = dict(r)
            for k, v in record.items():
                if k == "signals":
                    merged.setdefault("signals", {})
                    merged["signals"].update(v or {})
                else:
                    if v is not None:
                        merged[k] = v
            hist[i] = merged
            break
    else:
        hist.append(record)

    hist.sort(key=lambda x: x.get("date"))
    if len(hist) > MAX_HISTORY_DAYS:
        state["history"] = hist[-MAX_HISTORY_DAYS:]


def ma_ratio(values: List[float], window: int = MA_DAYS) -> Optional[float]:
    if len(values) < window:
        return None
    s = pd.Series(values, dtype="float64")
    ma = float(s.tail(window).mean())
    if ma == 0:
        return None
    return float(s.iloc[-1] / ma)


def days_to_2nd_friday(today: date) -> int:
    y, m = today.year, today.month
    first = date(y, m, 1)
    days_to_fri = (4 - first.weekday()) % 7  # 4=Fri
    first_fri = first + timedelta(days=days_to_fri)
    second_fri = first_fri + timedelta(days=7)
    return (second_fri - today).days


def is_sq_near(today: date) -> Tuple[bool, int]:
    d = days_to_2nd_friday(today)
    return (0 <= d <= SQ_NEAR_DAYS), d


# ========= JPX è£å®šå–å¼• (PDF Version) =========
def fetch_latest_arbitrage_pdf_url(s: requests.Session) -> Tuple[date, str]:
    """
    JPXã®ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°ã®ã€Œè£å®šå–å¼•ã€PDFã®URLã‚’å–å¾—ã™ã‚‹ã€‚
    HTMLã®è¡Œã‚„ãƒªãƒ³ã‚¯æ§‹é€ ã‚’èµ°æŸ»ã—ã¦æ—¥ä»˜ã¨PDFãƒªãƒ³ã‚¯ã®ãƒšã‚¢ã‚’æ¢ã™ã€‚
    """
    r = s.get(JPX_PROGRAM_URL, timeout=30)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "lxml")

    candidates = []

    # Method 1: Row Scanning (è¡Œã”ã¨ã®èµ°æŸ»)
    for tr in soup.find_all("tr"):
        text = tr.get_text(" ", strip=True)
        # ç©ºç™½æ–‡å­—ã‚’æ­£è¦åŒ–
        text = re.sub(r"\s+", " ", text)
        
        # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³: "2026å¹´1æœˆ16æ—¥" or "2026/01/16"
        m = re.search(r"(\d{4})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥", text)
        if not m:
            m = re.search(r"(\d{4})/\s*(\d{1,2})/\s*(\d{1,2})", text)
            
        if m:
            y, mo, d = map(int, m.groups())
            dt = date(y, mo, d)
            # åŒã˜è¡Œå†…ã®PDFãƒªãƒ³ã‚¯ã‚’æ¢ã™
            link = tr.find("a", href=re.compile(r"\.pdf", re.IGNORECASE))
            if link:
                url = _abs_url(JPX_PROGRAM_URL, link["href"])
                candidates.append((dt, url))
                continue

    # Method 2: Filename Scanning (ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã®æ¨å®š)
    if not candidates:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if not re.search(r"\.pdf", href, re.IGNORECASE):
                continue
            
            url = _abs_url(JPX_PROGRAM_URL, href)
            filename = href.split("/")[-1]
            
            # Pattern C: 20260116.pdf
            m8 = re.search(r"(20\d{2})(\d{2})(\d{2})", filename)
            if m8:
                y, mo, d = map(int, m8.groups())
                candidates.append((date(y, mo, d), url))
                continue
            
            # Pattern D: 260116.pdf (YYMMDD)
            m6 = re.search(r"(\d{2})(\d{2})(\d{2})", filename)
            if m6:
                y_short, mo, d = map(int, m6.groups())
                if 1 <= mo <= 12 and 1 <= d <= 31:
                    y = 2000 + y_short
                    candidates.append((date(y, mo, d), url))
                    continue

    if not candidates:
        raise RuntimeError("JPX program page: No arbitrage PDF links found.")

    # æ—¥ä»˜ã®é™é †ã§ã‚½ãƒ¼ãƒˆã—ã€æœ€æ–°ã®ã‚‚ã®ã‚’è¿”ã™
    candidates.sort(key=lambda x: x[0], reverse=True)
    return candidates[0]


def download_bytes(s: requests.Session, url: str) -> bytes:
    r = s.get(url, timeout=60)
    r.raise_for_status()
    return r.content


def extract_arbitrage_data_from_pdf(pdf_bytes: bytes) -> Tuple[float, float]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ­ã‚¸ãƒƒã‚¯ã«åŸºã¥ã„ã¦PDFã‹ã‚‰ã€Œè£å®šå£²ã‚Šæ®‹ã€ã€Œè£å®šè²·ã„æ®‹ã€ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    1. pdfplumberã§è¡¨ã‚’æŠ½å‡º
    2. 1åˆ—ç›®ãŒã€Œæ ªæ•°ã€ã§ã‚ã‚‹è¡Œã‚’æ¢ã™
    3. ã‚«ãƒ³ãƒã¨ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»ã—ã¦æ•°å€¤åŒ–
    4. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åŸºã¥ã„ã¦ã€å£²ã‚Šæ®‹ï¼ˆå‰æ–¹ï¼‰ã¨è²·ã„æ®‹ï¼ˆå¾Œæ–¹ï¼‰ã‚’å–å¾—
    """
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for page in pdf.pages:
            tables = page.extract_tables()
            for table in tables:
                for row in table:
                    # Noneã‚„æ”¹è¡Œã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    clean_row = [str(cell).replace("\n", "").strip() if cell else "" for cell in row]
                    
                    # 1. è¡Œã®ç‰¹å®š: 1åˆ—ç›®ãŒã€Œæ ªæ•°ã€
                    if not clean_row or "æ ªæ•°" not in clean_row[0]:
                        continue

                    # 2. å€¤ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ•°å€¤åŒ–
                    nums = []
                    for cell in clean_row:
                        # ã‚«ãƒ³ãƒã¨ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
                        val_text = cell.replace(",", "").replace(" ", "")
                        # æ•°å­—éƒ¨åˆ†ã‚’æŠ½å‡º (ä¾‹: "69381" "1047021" ãªã©)
                        # å°æ•°ç‚¹ã‚’å«ã‚€å¯èƒ½æ€§ã‚‚è€ƒæ…®ã—ã¦æŠ½å‡º
                        ms = re.findall(r"(\d+(?:\.\d+)?)", val_text)
                        for m in ms:
                            try:
                                nums.append(float(m))
                            except ValueError:
                                pass
                    
                    # 3. åˆ—ã®ãƒãƒƒãƒ”ãƒ³ã‚° (æŒ‡å®šãƒ­ã‚¸ãƒƒã‚¯)
                    # numsã«ã¯ [æ ªæ•°(ã‚‚ã—æ•°å€¤ãªã‚‰), å£²ã‚Šå½“é™, å£²ã‚Šåˆè¨ˆ, å£²ã‚Šç¿Œé™, è²·ã„å½“é™, è²·ã„ç¿Œé™, è²·ã„åˆè¨ˆ] ã®é †ã§å…¥ã‚‹ã¯ãš
                    # ä¾‹: [69381, 69381, 0, 1047021, 1762, 1048783]
                    
                    if len(nums) >= 2:
                        # ä¸€ç•ªæœ€å¾Œã®æ•°å€¤ = ã€Œè²·ã„æ®‹ï¼ˆåˆè¨ˆï¼‰ã€
                        arb_buy = nums[-1]
                        
                        # å‰ã‹ã‚‰2ç•ªç›®(ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹1)ã®æ•°å€¤ = ã€Œå£²ã‚Šæ®‹ï¼ˆåˆè¨ˆï¼‰ã€
                        # â€»nums[0]ã¯ã€Œå£²ã‚Šå½“é™ã€ã€nums[1]ã¯ã€Œå£²ã‚Šåˆè¨ˆã€ã¨ãªã‚‹ã®ãŒä¸€èˆ¬çš„
                        # ãƒªã‚¹ãƒˆã®é•·ã•ãŒååˆ†ã«ã‚ã‚‹ã‹ç¢ºèª
                        if len(nums) >= 2:
                            arb_sell = nums[1]
                        else:
                            # ä¸‡ãŒä¸€æ•°å€¤ãŒ2ã¤ã—ã‹ãªã„å ´åˆã¯å…ˆé ­ã‚’å£²ã‚Šæ®‹ã¨ã™ã‚‹
                            arb_sell = nums[0]
                            
                        return arb_buy, arb_sell

    raise RuntimeError("Arbitrage data (Buy/Sell positions) not found in PDF tables using 'Share count' row logic.")


# ========= JPX æ—¥å ±ï¼ˆãƒ—ãƒ©ã‚¤ãƒ å£²è²·é«˜ï¼‰(Robust Ver.) =========
def fetch_latest_daily_pdf_url(s: requests.Session) -> Tuple[date, str]:
    r = s.get(JPX_DAILY_URL, timeout=30)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "lxml")

    candidates = []

    # Method 1: Row Scanning
    for tr in soup.find_all("tr"):
        text = tr.get_text(" ", strip=True)
        text = re.sub(r"\s+", " ", text)
        
        m = re.search(r"(\d{4})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥", text)
        if not m:
            m = re.search(r"(\d{4})/\s*(\d{1,2})/\s*(\d{1,2})", text)
        
        if m:
            y, mo, d = map(int, m.groups())
            dt = date(y, mo, d)
            link = tr.find("a", href=re.compile(r"\.pdf", re.IGNORECASE))
            if link:
                url = _abs_url(JPX_DAILY_URL, link["href"])
                candidates.append((dt, url))
                continue

    # Method 2: Filename Scanning
    if not candidates:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if not re.search(r"\.pdf", href, re.IGNORECASE):
                continue
            
            url = _abs_url(JPX_DAILY_URL, href)
            filename = href.split("/")[-1]
            
            # Pattern C: 20260116.pdf
            m8 = re.search(r"(20\d{2})(\d{2})(\d{2})", filename)
            if m8:
                y, mo, d = map(int, m8.groups())
                candidates.append((date(y, mo, d), url))
                continue

            # Pattern D: 260116.pdf (YYMMDD)
            m6 = re.search(r"(\d{2})(\d{2})(\d{2})", filename)
            if m6:
                y_short, mo, d = map(int, m6.groups())
                if 1 <= mo <= 12 and 1 <= d <= 31:
                    y = 2000 + y_short
                    candidates.append((date(y, mo, d), url))
                    continue

    if not candidates:
        raise RuntimeError("JPX daily report page: No PDF links found.")

    candidates.sort(key=lambda x: x[0], reverse=True)
    return candidates[0]


def extract_prime_volume_from_pdf(pdf_bytes: bytes) -> float:
    """
    æ—¥å ±PDFã‹ã‚‰ã€Œãƒ—ãƒ©ã‚¤ãƒ å¸‚å ´ã€ã®å£²è²·é«˜ã‚’å–å¾—ã™ã‚‹ã€‚
    ã“ã“ã§ã‚‚è¡¨è§£æ(extract_tables)ã‚’å„ªå…ˆã—ã€ã ã‚ãªã‚‰ãƒ†ã‚­ã‚¹ãƒˆè§£æã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
    """
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        # 1. Table Extraction Strategy
        for page in pdf.pages:
            tables = page.extract_tables()
            for table in tables:
                for row in table:
                    # è¡Œã‚’æ­£è¦åŒ–ã—ã¦çµåˆ
                    clean_row = [str(x).replace("\n", "").strip() if x else "" for x in row]
                    row_text = "".join(clean_row)
                    
                    if "ãƒ—ãƒ©ã‚¤ãƒ " in row_text or "Prime" in row_text:
                        # æ•°å€¤æŠ½å‡º
                        nums = []
                        for cell in clean_row:
                            # 1,234 or 1234.56
                            ms = re.findall(r"(\d{1,3}(?:,\d{3})*(?:\.\d+)?)", cell)
                            for m in ms:
                                try:
                                    val = float(m.replace(",", ""))
                                    nums.append(val)
                                except:
                                    continue
                        if nums:
                            # ä¸€èˆ¬çš„ã«å£²è²·é«˜ãƒ»å£²è²·ä»£é‡‘ã®ä¸­ã§æœ€å¤§å€¤ã‚’æ¡ç”¨ã™ã‚Œã°å¤§ããå¤–ã•ãªã„
                            return max(nums)

        # 2. Text Extraction Strategy (Fallback)
        for page in pdf.pages:
            text = page.extract_text() or ""
            for line in text.splitlines():
                if "ãƒ—ãƒ©ã‚¤ãƒ " in line or "Prime" in line:
                    matches = re.findall(r"(\d{1,3}(?:,\d{3})*(?:\.\d+)?)", line)
                    nums = []
                    for m in matches:
                        try:
                            val = float(m.replace(",", ""))
                            nums.append(val)
                        except:
                            continue
                    if nums:
                        return max(nums)

    raise RuntimeError("Prime volume not found in daily PDF text")


# ========= æŒ‡æ•°ï¼ˆéå»3å¹´ã®é«˜å€¤åœï¼‰ =========
def fetch_index_high_zone(ticker: str, pctl: float, lookback: str) -> Dict:
    """
    éå»3å¹´ã®çµ‚å€¤åˆ†å¸ƒã«å¯¾ã—ã¦ã€æœ€æ–°çµ‚å€¤ãŒpctlä»¥ä¸Šã‹ã‚’åˆ¤å®šã€‚
    """
    df = yf.download(ticker, period=lookback, interval="1d", auto_adjust=False, progress=False)
    if df is None or df.empty:
        raise RuntimeError(f"yfinance: no data for ticker={ticker}")

    if "Close" not in df.columns:
        raise RuntimeError(f"yfinance: Close not found for ticker={ticker}")

    close = df["Close"].dropna()
    if close.empty:
        raise RuntimeError(f"yfinance: Close empty for ticker={ticker}")

    latest_close = float(close.iloc[-1])
    q = float(close.quantile(pctl))
    high_zone = latest_close >= q

    return {
        "ticker": ticker,
        "lookback": lookback,
        "pctl": pctl,
        "latest_close": latest_close,
        "threshold_close": q,
        "index_high_zone": bool(high_zone),
        "index_latest_date": close.index[-1].date().isoformat(),
    }


def compute_latest(state: Dict, index_info: Dict) -> Dict:
    hist = state["history"]

    arb_days = [r for r in hist if isinstance(r.get("arb_buy"), (int, float))]
    vol_days = [r for r in hist if isinstance(r.get("prime_volume"), (int, float))]

    latest = {
        "asof": datetime.now().astimezone().isoformat(),
        "inputs": {
            "arb_date": None,
            "arb_buy": None,
            "arb_sell": None,
            "prime_volume_date": None,
            "prime_volume": None,
            "index": index_info,
        },
        "metrics": {
            "arb_buy_ratio_ma20": None,
            "prime_volume_ratio_ma20": None,
            "days_to_2nd_fri": None,
            "index_latest_close": index_info["latest_close"],
            "index_threshold_close_pctl": index_info["threshold_close"],
        },
        "thresholds": {
            "arb_buy_ratio_ma20_hot": ARB_BUY_RATIO_TH,
            "prime_volume_ratio_ma20_thin": PRIME_VOL_RATIO_TH,
            "sq_near_days": SQ_NEAR_DAYS,
            "index_pctl": INDEX_PCTL,
        },
        "conditions": {
            "arb_buy_hot": False,
            "sq_near": False,
            "prime_volume_thin": False,
            "index_high_zone": bool(index_info["index_high_zone"]),
        },
        "alert": {
            "volatility_risk": False,
            "rule": "arb_buy_hot & sq_near & prime_volume_thin & index_high_zone",
            "reasons": [],
        },
    }

    # --- arbitrage condition ---
    if arb_days:
        arb_days.sort(key=lambda x: x["date"])
        series = [float(r["arb_buy"]) for r in arb_days]
        ratio = ma_ratio(series, MA_DAYS)
        arb_latest = arb_days[-1]
        today = date.fromisoformat(arb_latest["date"])
        sq_near, d2f = is_sq_near(today)

        latest["inputs"]["arb_date"] = arb_latest["date"]
        latest["inputs"]["arb_buy"] = float(arb_latest["arb_buy"])
        latest["inputs"]["arb_sell"] = float(arb_latest["arb_sell"])

        latest["metrics"]["arb_buy_ratio_ma20"] = ratio
        latest["metrics"]["days_to_2nd_fri"] = d2f

        arb_hot = ratio is not None and ratio >= ARB_BUY_RATIO_TH
        latest["conditions"]["arb_buy_hot"] = bool(arb_hot)
        latest["conditions"]["sq_near"] = bool(sq_near)

    # --- volume condition ---
    if vol_days:
        vol_days.sort(key=lambda x: x["date"])
        series = [float(r["prime_volume"]) for r in vol_days]
        ratio = ma_ratio(series, MA_DAYS)
        vol_latest = vol_days[-1]

        latest["inputs"]["prime_volume_date"] = vol_latest["date"]
        latest["inputs"]["prime_volume"] = float(vol_latest["prime_volume"])

        latest["metrics"]["prime_volume_ratio_ma20"] = ratio

        vol_thin = ratio is not None and ratio <= PRIME_VOL_RATIO_TH
        latest["conditions"]["prime_volume_thin"] = bool(vol_thin)

    # --- alert ---
    c = latest["conditions"]
    alert = c["arb_buy_hot"] and c["sq_near"] and c["prime_volume_thin"] and c["index_high_zone"]
    latest["alert"]["volatility_risk"] = bool(alert)
    latest["alert"]["reasons"] = [k for k, v in c.items() if v]

    state["latest"] = latest
    return latest


def main() -> None:
    s = sess()
    state = load_state()

    # 1) è£å®šæ®‹ï¼ˆæœ€æ–°åˆ†ï¼‰ PDFç‰ˆ
    # ã“ã“ã§Excelç”¨é–¢æ•°ã§ã¯ãªãã€PDFç”¨é–¢æ•°ã‚’å‘¼ã¶
    arb_dt, arb_url = fetch_latest_arbitrage_pdf_url(s)
    arb_pdf_bytes = download_bytes(s, arb_url)
    arb_buy, arb_sell = extract_arbitrage_data_from_pdf(arb_pdf_bytes)

    upsert_history(
        state,
        {
            "date": arb_dt.isoformat(),
            "arb_buy": arb_buy,
            "arb_sell": arb_sell,
            "arb_net": arb_buy - arb_sell,
            "prime_volume": None,
            "signals": {},
            "src": {"arb_pdf": arb_url},
        },
    )

    # 2) æ—¥å ±ï¼ˆæœ€æ–°åˆ†ï¼‰ãƒ—ãƒ©ã‚¤ãƒ å£²è²·é«˜
    vol_dt, pdf_url = fetch_latest_daily_pdf_url(s)
    pdf_bytes = download_bytes(s, pdf_url)
    prime_volume = extract_prime_volume_from_pdf(pdf_bytes)

    upsert_history(
        state,
        {
            "date": vol_dt.isoformat(),
            "arb_buy": None,
            "arb_sell": None,
            "arb_net": None,
            "prime_volume": prime_volume,
            "signals": {},
            "src": {"daily_pdf": pdf_url},
        },
    )

    # 3) æŒ‡æ•°é«˜å€¤åœ
    index_info = fetch_index_high_zone(INDEX_TICKER, INDEX_PCTL, INDEX_LOOKBACK)

    # 4) åˆ¤å®šã¾ã¨ã‚ï¼ˆlatestï¼‰
    latest = compute_latest(state, index_info)
    save_state(state)

    # 5) runãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ¬ãƒãƒ¼ãƒˆå½¢å¼ï¼‰
    print_report(latest)


if __name__ == "__main__":
    main()
