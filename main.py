from __future__ import annotations

import io
import json
import os
import re
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
import requests
from bs4 import BeautifulSoup
import yfinance as yf

STATE_PATH = Path("state.json")

# ====== ä»•æ§˜ï¼ˆã‚ãªãŸã®æ±ºå®šï¼‰ ======
MA_DAYS = 20
MAX_HISTORY_DAYS = 900  # 3å¹´ â‰’ 756å–¶æ¥­æ—¥ + ãƒãƒƒãƒ•ã‚¡

ARB_BUY_RATIO_TH = 1.5
PRIME_VOL_RATIO_TH = 0.85
SQ_NEAR_DAYS = 5

# æŒ‡æ•°é«˜å€¤åœï¼ˆéå»3å¹´ï¼‰åˆ¤å®š
INDEX_PCTL = 0.90  # 90%ç‚¹
INDEX_TICKER = os.getenv("INDEX_TICKER", "^N225")  # ãƒ‡ãƒ•ã‚©: æ—¥çµŒ225ã€‚TOPIXã«ã—ãŸã‘ã‚Œã° ^TOPX ç­‰ã‚’è¨­å®š
INDEX_LOOKBACK = "3y"

JPX_PROGRAM_URL = "https://www.jpx.co.jp/markets/statistics-equities/program/index.html"
JPX_DAILY_URL = "https://www.jpx.co.jp/markets/statistics-equities/daily/index.html"

UA = "Mozilla/5.0 (compatible; jpx-bot/1.0; +https://github.com/)"


def fmt_bool(x: bool) -> str:
    return "TRUE" if x else "FALSE"


def fmt_num(x) -> str:
    if x is None:
        return "N/A"
    try:
        return f"{float(x):,.4f}"
    except Exception:
        return str(x)


def pick_level(alert: bool, conds: Dict[str, bool]) -> Tuple[str, str]:
    """
    LEVELã®ãƒ«ãƒ¼ãƒ«ï¼ˆå›ºå®šï¼‰:
      - LEVEL 3: ALERT=True
      - LEVEL 2: ALERT=False ã‹ã¤ æ¡ä»¶ãŒ2ã¤ä»¥ä¸ŠTRUE
      - LEVEL 1: ãã‚Œä»¥å¤–
    """
    true_cnt = sum(1 for v in conds.values() if v)
    if alert:
        return (
            "LEVEL 3: WARNING (è­¦æˆ’)",
            "ã€è­¦æˆ’ã€‘æ€¥å¤‰ã—ã‚„ã™ã„æ¡ä»¶ãŒæƒã£ã¦ã„ã¾ã™ã€‚å»ºç‰ã‚µã‚¤ã‚ºãƒ»æ–°è¦æŠ•å…¥ã‚’æŠ‘ãˆã€SQé€±ã¯ç‰¹ã«æ…é‡ã«ã€‚",
        )
    if true_cnt >= 2:
        return (
            "LEVEL 2: CAUTION (æ³¨æ„)",
            "ã€æ³¨æ„ã€‘ä¸€éƒ¨ã®æ­ªã¿ãŒå‡ºã¦ã„ã¾ã™ã€‚ç„¡ç†ãªè²·ã„æ–¹ï¼ˆãƒ¬ãƒ/ä¸€æ‹¬ï¼‰ã‚’é¿ã‘ã€åˆ†å‰²ã¨ä½™åŠ›é‡è¦–ã€‚",
        )
    return ("LEVEL 1: NORMAL (æ­£å¸¸)", "ã€é †è¡Œã€‘æ§‹é€ çš„ãªå±æ©Ÿæ¡ä»¶ã¯æœªæˆç«‹ã€‚é€šå¸¸é‹ç”¨ã§å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚")


def print_report(latest: Dict):
    idx = latest["inputs"]["index"]
    conds = latest["conditions"]
    metrics = latest["metrics"]
    thr = latest["thresholds"]
    alert = latest["alert"]["volatility_risk"]

    level_title, headline = pick_level(alert, conds)

    print("#" * 60)
    print(f"   {level_title}")
    print("#" * 60)
    print("")
    print("[ç·åˆåˆ¤å®šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]")
    print(headline)
    print("")
    print("#" * 60)
    print("=" * 60)
    print("ğŸ“Š JPX è£å®šãƒ»SQãƒ»æµå‹•æ€§ãƒ¬ãƒãƒ¼ãƒˆ (v1.0)")
    print("=" * 60)
    print(f"AsOf: {latest['asof']}")
    print("")
    print("[å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜]")
    print(f"- è£å®šå–å¼•ï¼ˆJPXï¼‰: {latest['inputs']['arb_date']}  â€»JPXã¯é…å»¶ã®å¯èƒ½æ€§ã‚ã‚Š")
    print(f"- ãƒ—ãƒ©ã‚¤ãƒ å‡ºæ¥é«˜ï¼ˆJPXæ—¥å ±ï¼‰: {latest['inputs']['prime_volume_date']}")
    print(f"- æŒ‡æ•°ï¼ˆ{idx['ticker']}ï¼‰: {idx['index_latest_date']}ï¼ˆçµ‚å€¤ãƒ™ãƒ¼ã‚¹ï¼‰")
    print("-" * 60)
    print("")

    # 1) Arbitrage
    print("1. Condition: Arbitrage Stretchï¼ˆè£å®šè²·ã„æ®‹ã®ç©ã¿ä¸ŠãŒã‚Šï¼‰")
    print(
        f"   çµæœ: {fmt_num(metrics['arb_buy_ratio_ma20'])}  (é–¾å€¤: >= {thr['arb_buy_ratio_ma20_hot']}) â†’ [{fmt_bool(conds['arb_buy_hot'])}]"
    )
    print("   [åˆ†æ]:")
    if metrics["arb_buy_ratio_ma20"] is None:
        print("   - ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆMA20æœªæˆç«‹ï¼‰ã€‚20å–¶æ¥­æ—¥åˆ†ãŒè²¯ã¾ã‚‹ã¾ã§åˆ¤å®šä¿ç•™ã€‚")
    elif conds["arb_buy_hot"]:
        print("   - è£å®šè²·ã„æ®‹ãŒå¹³å¸¸ã‚ˆã‚Šå¤§ããã€è§£æ¶ˆãŒèµ°ã‚‹ã¨ç¾ç‰©å£²ã‚Šåœ§ãŒå‡ºã‚„ã™ã„çŠ¶æ…‹ã§ã™ã€‚")
    else:
        print("   - è£å®šè²·ã„æ®‹ã¯å¹³å¸¸ãƒ¬ãƒ³ã‚¸ã€‚éœ€çµ¦ã®â€œç«è–¬åº«â€ã¯å¤§ããã‚ã‚Šã¾ã›ã‚“ã€‚")
    print("")

    # 2) SQ near
    print("2. Trigger: SQ Nearï¼ˆSQæ¥è¿‘ï¼‰")
    print(
        f"   çµæœ: days_to_2nd_fri = {metrics['days_to_2nd_fri']}  (é–¾å€¤: <= {thr['sq_near_days']}) â†’ [{fmt_bool(conds['sq_near'])}]"
    )
    print("   [åˆ†æ]:")
    if metrics["days_to_2nd_fri"] is None:
        print("   - æ—¥ä»˜è¨ˆç®—ã«å¤±æ•—ï¼ˆæƒ³å®šå¤–ï¼‰ã€‚")
    elif conds["sq_near"]:
        print("   - ä¾¡æ ¼å·®ãŒç· ã¾ã‚Šã‚„ã™ã„æœŸé–“ã€‚è£å®šã®è§£æ¶ˆãŒåŒæ–¹å‘ã«å‡ºã‚‹ã¨å€¤ãŒé£›ã³ã‚„ã™ã„ã€‚")
    else:
        print("   - SQã¯è¿‘ãã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¤ãƒ™ãƒ³ãƒˆè¦å› ã¯å¼±ã„ã€‚")
    print("")

    # 3) Prime volume thin
    print("3. Trigger: Prime Liquidity Thinï¼ˆãƒ—ãƒ©ã‚¤ãƒ æµå‹•æ€§ã®è–„ã•ï¼‰")
    print(
        f"   çµæœ: {fmt_num(metrics['prime_volume_ratio_ma20'])}  (é–¾å€¤: <= {thr['prime_volume_ratio_ma20_thin']}) â†’ [{fmt_bool(conds['prime_volume_thin'])}]"
    )
    print("   [åˆ†æ]:")
    if metrics["prime_volume_ratio_ma20"] is None:
        print("   - ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆMA20æœªæˆç«‹ï¼‰ã€‚20å–¶æ¥­æ—¥åˆ†ãŒè²¯ã¾ã‚‹ã¾ã§åˆ¤å®šä¿ç•™ã€‚")
    elif conds["prime_volume_thin"]:
        print("   - å¸‚å ´ã®å—ã‘çš¿ãŒè–„ã„ã€‚å°ã•ãªè§£æ¶ˆã§ã‚‚å€¤ãŒæ»‘ã‚Šã‚„ã™ã„å±€é¢ã§ã™ã€‚")
    else:
        print("   - å‡ºæ¥é«˜ã¯å¹³å¸¸åŸŸã€‚å—ã‘çš¿ã¯æ¥µç«¯ã«è–„ãã‚ã‚Šã¾ã›ã‚“ã€‚")
    print("")

    # 4) Index high zone
    print(f"4. Condition: Index High Zoneï¼ˆæŒ‡æ•°ã®é«˜å€¤åœï¼šéå»3å¹´ p{int(thr['index_pctl']*100)}ï¼‰")
    print(
        f"   çµæœ: latest_close={fmt_num(idx['latest_close'])}, threshold={fmt_num(idx['threshold_close'])} â†’ [{fmt_bool(conds['index_high_zone'])}]"
    )
    print("   [åˆ†æ]:")
    if conds["index_high_zone"]:
        print("   - ä¾¡æ ¼ä½ç½®ãŒä¸Šå´ã«å¯„ã£ã¦ã„ã¾ã™ã€‚å´©ã‚Œã‚‹ã¨ãã®ä¸‹æ–¹å‘ã®æŒ¯ã‚ŒãŒå‡ºã‚„ã™ã„å´ã§ã™ã€‚")
    else:
        print("   - é«˜å€¤åœã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ä¾¡æ ¼ä½ç½®ã®â€œä¸Šè©°ã¾ã‚Šâ€è¦å› ã¯å¼±ã„ã€‚")
    print("-" * 60)

    print("")
    print("[æœ€çµ‚åˆ¤å®š]")
    print(f"ALERT_VOLATILITY_RISK = {fmt_bool(alert)}")
    print(f"Rule: {latest['alert']['rule']}")
    if alert:
        print("æˆç«‹æ¡ä»¶:")
        for k in latest["alert"]["reasons"]:
            print(f"- {k}")
    print("")


def sess() -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": UA})
    return s


def _abs_url(base: str, href: str) -> str:
    if href.startswith("http"):
        return href
    if href.startswith("/"):
        return "https://www.jpx.co.jp" + href
    base_dir = base.rsplit("/", 1)[0] + "/"
    return base_dir + href


def load_state() -> Dict:
    if STATE_PATH.exists():
        return json.loads(STATE_PATH.read_text(encoding="utf-8"))
    return {
        "meta": {"created_at": datetime.now().isoformat(), "updated_at": None, "version": 2},
        "history": [],
        "latest": {},
    }


def save_state(state: Dict) -> None:
    state["meta"]["updated_at"] = datetime.now().isoformat()
    STATE_PATH.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")


def upsert_history(state: Dict, record: Dict) -> None:
    ds = record["date"]
    hist = state["history"]

    for i, r in enumerate(hist):
        if r.get("date") == ds:
            merged = dict(r)
            for k, v in record.items():
                if k == "signals":
                    merged.setdefault("signals", {})
                    merged["signals"].update(v or {})
                else:
                    if v is not None:
                        merged[k] = v
            hist[i] = merged
            break
    else:
        hist.append(record)

    hist.sort(key=lambda x: x.get("date"))
    if len(hist) > MAX_HISTORY_DAYS:
        state["history"] = hist[-MAX_HISTORY_DAYS:]


def ma_ratio(values: List[float], window: int = MA_DAYS) -> Optional[float]:
    if len(values) < window:
        return None
    s = pd.Series(values, dtype="float64")
    ma = float(s.tail(window).mean())
    if ma == 0:
        return None
    return float(s.iloc[-1] / ma)


def days_to_2nd_friday(today: date) -> int:
    y, m = today.year, today.month
    first = date(y, m, 1)
    days_to_fri = (4 - first.weekday()) % 7  # 4=Fri
    first_fri = first + timedelta(days=days_to_fri)
    second_fri = first_fri + timedelta(days=7)
    return (second_fri - today).days


def is_sq_near(today: date) -> Tuple[bool, int]:
    d = days_to_2nd_friday(today)
    return (0 <= d <= SQ_NEAR_DAYS), d


# ========= JPX è£å®šå–å¼• (Robust Ver. 3) =========
def fetch_latest_arbitrage_excel_url(s: requests.Session) -> Tuple[date, str]:
    r = s.get(JPX_PROGRAM_URL, timeout=30)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "lxml")

    candidates = []

    # Method 1: Row Scanning (Text based)
    for tr in soup.find_all("tr"):
        text = tr.get_text(" ", strip=True)
        text = re.sub(r"\s+", " ", text)
        
        # Pattern A: 2026å¹´1æœˆ16æ—¥
        m = re.search(r"(\d{4})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥", text)
        if not m:
            m = re.search(r"(\d{4})/\s*(\d{1,2})/\s*(\d{1,2})", text)
            
        if m:
            y, mo, d = map(int, m.groups())
            dt = date(y, mo, d)
            link = tr.find("a", href=re.compile(r"\.xls", re.IGNORECASE))
            if link:
                url = _abs_url(JPX_PROGRAM_URL, link["href"])
                candidates.append((dt, url))
                continue

    # Method 2: Filename Scanning (Fallback)
    if not candidates:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if not re.search(r"\.xls", href, re.IGNORECASE):
                continue
            
            url = _abs_url(JPX_PROGRAM_URL, href)
            filename = href.split("/")[-1]
            
            # Pattern C: 20260116.xls
            m8 = re.search(r"(20\d{2})(\d{2})(\d{2})", filename)
            if m8:
                y, mo, d = map(int, m8.groups())
                candidates.append((date(y, mo, d), url))
                continue
            
            # Pattern D: 260116.xls (YYMMDD)
            m6 = re.search(r"(\d{2})(\d{2})(\d{2})", filename)
            if m6:
                y_short, mo, d = map(int, m6.groups())
                if 1 <= mo <= 12 and 1 <= d <= 31:
                    y = 2000 + y_short
                    candidates.append((date(y, mo, d), url))
                    continue

    if not candidates:
        raise RuntimeError("JPX program page: No arbitrage excel links found (all methods failed).")

    candidates.sort(key=lambda x: x[0], reverse=True)
    return candidates[0]


def download_bytes(s: requests.Session, url: str) -> bytes:
    r = s.get(url, timeout=60)
    r.raise_for_status()
    return r.content


def parse_arbitrage_excel(excel_bytes: bytes) -> Tuple[float, float]:
    # Check if content is HTML (sometimes scraper gets blocked or 404)
    if excel_bytes.lstrip().startswith(b"<!DOCTYPE") or excel_bytes.lstrip().startswith(b"<html"):
        raise RuntimeError("Downloaded content appears to be HTML, not Excel. (Possible anti-bot block or 404)")

    bio = io.BytesIO(excel_bytes)

    # ä¿®æ­£: .xls (Binary) ã‚’èª­ã‚€å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ engine="openpyxl" ã‚’å¼·åˆ¶ã—ãªã„
    # â€» .xls ã‚’èª­ã‚€ã«ã¯ xlrd ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
    try:
        df = pd.read_excel(bio)
    except ImportError:
        raise RuntimeError("Parsing failed. For .xls files, please ensure 'xlrd' is installed (pip install xlrd>=2.0.1).")
    except Exception as e:
        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼ã‚’å‡ºã—ã¦ãƒ‡ãƒãƒƒã‚°ã—ã‚„ã™ãã™ã‚‹
        raise RuntimeError(f"Excel parsing failed: {e}")

    df.columns = [str(c).strip() for c in df.columns]

    def find_col(patterns: List[str]) -> Optional[str]:
        for c in df.columns:
            if any(p in c for p in patterns):
                return c
        return None

    buy_col = find_col(["è£å®šè²·", "è²·ã„æ®‹", "è²·æ®‹"])
    sell_col = find_col(["è£å®šå£²", "å£²ã‚Šæ®‹", "å£²æ®‹"])
    if not buy_col or not sell_col:
        raise RuntimeError(f"Arb columns not found: {list(df.columns)}")

    def first_number(series: pd.Series) -> float:
        for v in series.tolist():
            if pd.isna(v):
                continue
            try:
                return float(v)
            except Exception:
                continue
        raise RuntimeError("No numeric value found in arbitrage sheet")

    arb_buy = first_number(df[buy_col])
    arb_sell = first_number(df[sell_col])
    return arb_buy, arb_sell


# ========= JPX æ—¥å ±ï¼ˆãƒ—ãƒ©ã‚¤ãƒ å£²è²·é«˜ï¼‰(Robust Ver. 3) =========
def fetch_latest_daily_pdf_url(s: requests.Session) -> Tuple[date, str]:
    r = s.get(JPX_DAILY_URL, timeout=30)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "lxml")

    candidates = []

    # Method 1: Row Scanning
    for tr in soup.find_all("tr"):
        text = tr.get_text(" ", strip=True)
        text = re.sub(r"\s+", " ", text)
        
        m = re.search(r"(\d{4})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥", text)
        if not m:
            m = re.search(r"(\d{4})/\s*(\d{1,2})/\s*(\d{1,2})", text)
        
        if m:
            y, mo, d = map(int, m.groups())
            dt = date(y, mo, d)
            link = tr.find("a", href=re.compile(r"\.pdf", re.IGNORECASE))
            if link:
                url = _abs_url(JPX_DAILY_URL, link["href"])
                candidates.append((dt, url))
                continue

    # Method 2: Filename Scanning
    if not candidates:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if not re.search(r"\.pdf", href, re.IGNORECASE):
                continue
            
            url = _abs_url(JPX_DAILY_URL, href)
            filename = href.split("/")[-1]
            
            # Pattern C: 20260116.pdf
            m8 = re.search(r"(20\d{2})(\d{2})(\d{2})", filename)
            if m8:
                y, mo, d = map(int, m8.groups())
                candidates.append((date(y, mo, d), url))
                continue

            # Pattern D: 260116.pdf (YYMMDD)
            m6 = re.search(r"(\d{2})(\d{2})(\d{2})", filename)
            if m6:
                y_short, mo, d = map(int, m6.groups())
                if 1 <= mo <= 12 and 1 <= d <= 31:
                    y = 2000 + y_short
                    candidates.append((date(y, mo, d), url))
                    continue

    if not candidates:
        raise RuntimeError("JPX daily report page: No PDF links found (all methods failed).")

    candidates.sort(key=lambda x: x[0], reverse=True)
    return candidates[0]


def extract_prime_volume_from_pdf(pdf_bytes: bytes) -> float:
    import pdfplumber

    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        lines: List[str] = []
        for page in pdf.pages:
            t = page.extract_text() or ""
            for ln in t.splitlines():
                ln = ln.strip()
                if ln:
                    lines.append(ln)

    keys = ["ãƒ—ãƒ©ã‚¤ãƒ ", "Prime", "æ±è¨¼ãƒ—ãƒ©ã‚¤ãƒ "]
    for ln in lines:
        if any(k in ln for k in keys):
            nums = re.findall(r"\d{1,3}(?:,\d{3})+(?:\.\d+)?|\d+(?:\.\d+)?", ln)
            if not nums:
                continue
            vals = []
            for s in nums:
                try:
                    vals.append(float(s.replace(",", "")))
                except Exception:
                    pass
            if vals:
                return max(vals)

    raise RuntimeError("Prime volume not found in daily PDF text")


# ========= æŒ‡æ•°ï¼ˆéå»3å¹´ã®é«˜å€¤åœï¼‰ =========
def fetch_index_high_zone(ticker: str, pctl: float, lookback: str) -> Dict:
    """
    éå»3å¹´ã®çµ‚å€¤åˆ†å¸ƒã«å¯¾ã—ã¦ã€æœ€æ–°çµ‚å€¤ãŒpctlä»¥ä¸Šã‹ã‚’åˆ¤å®šã€‚
    """
    df = yf.download(ticker, period=lookback, interval="1d", auto_adjust=False, progress=False)
    if df is None or df.empty:
        raise RuntimeError(f"yfinance: no data for ticker={ticker}")

    if "Close" not in df.columns:
        raise RuntimeError(f"yfinance: Close not found for ticker={ticker}")

    close = df["Close"].dropna()
    if close.empty:
        raise RuntimeError(f"yfinance: Close empty for ticker={ticker}")

    latest_close = float(close.iloc[-1])
    q = float(close.quantile(pctl))
    high_zone = latest_close >= q

    return {
        "ticker": ticker,
        "lookback": lookback,
        "pctl": pctl,
        "latest_close": latest_close,
        "threshold_close": q,
        "index_high_zone": bool(high_zone),
        "index_latest_date": close.index[-1].date().isoformat(),
    }


def compute_latest(state: Dict, index_info: Dict) -> Dict:
    hist = state["history"]

    arb_days = [r for r in hist if isinstance(r.get("arb_buy"), (int, float))]
    vol_days = [r for r in hist if isinstance(r.get("prime_volume"), (int, float))]

    latest = {
        "asof": datetime.now().astimezone().isoformat(),
        "inputs": {
            "arb_date": None,
            "arb_buy": None,
            "arb_sell": None,
            "prime_volume_date": None,
            "prime_volume": None,
            "index": index_info,
        },
        "metrics": {
            "arb_buy_ratio_ma20": None,
            "prime_volume_ratio_ma20": None,
            "days_to_2nd_fri": None,
            "index_latest_close": index_info["latest_close"],
            "index_threshold_close_pctl": index_info["threshold_close"],
        },
        "thresholds": {
            "arb_buy_ratio_ma20_hot": ARB_BUY_RATIO_TH,
            "prime_volume_ratio_ma20_thin": PRIME_VOL_RATIO_TH,
            "sq_near_days": SQ_NEAR_DAYS,
            "index_pctl": INDEX_PCTL,
        },
        "conditions": {
            "arb_buy_hot": False,
            "sq_near": False,
            "prime_volume_thin": False,
            "index_high_zone": bool(index_info["index_high_zone"]),
        },
        "alert": {
            "volatility_risk": False,
            "rule": "arb_buy_hot & sq_near & prime_volume_thin & index_high_zone",
            "reasons": [],
        },
    }

    # --- arbitrage condition ---
    if arb_days:
        arb_days.sort(key=lambda x: x["date"])
        series = [float(r["arb_buy"]) for r in arb_days]
        ratio = ma_ratio(series, MA_DAYS)
        arb_latest = arb_days[-1]
        today = date.fromisoformat(arb_latest["date"])
        sq_near, d2f = is_sq_near(today)

        latest["inputs"]["arb_date"] = arb_latest["date"]
        latest["inputs"]["arb_buy"] = float(arb_latest["arb_buy"])
        latest["inputs"]["arb_sell"] = float(arb_latest["arb_sell"])

        latest["metrics"]["arb_buy_ratio_ma20"] = ratio
        latest["metrics"]["days_to_2nd_fri"] = d2f

        arb_hot = ratio is not None and ratio >= ARB_BUY_RATIO_TH
        latest["conditions"]["arb_buy_hot"] = bool(arb_hot)
        latest["conditions"]["sq_near"] = bool(sq_near)

    # --- volume condition ---
    if vol_days:
        vol_days.sort(key=lambda x: x["date"])
        series = [float(r["prime_volume"]) for r in vol_days]
        ratio = ma_ratio(series, MA_DAYS)
        vol_latest = vol_days[-1]

        latest["inputs"]["prime_volume_date"] = vol_latest["date"]
        latest["inputs"]["prime_volume"] = float(vol_latest["prime_volume"])

        latest["metrics"]["prime_volume_ratio_ma20"] = ratio

        vol_thin = ratio is not None and ratio <= PRIME_VOL_RATIO_TH
        latest["conditions"]["prime_volume_thin"] = bool(vol_thin)

    # --- alert ---
    c = latest["conditions"]
    alert = c["arb_buy_hot"] and c["sq_near"] and c["prime_volume_thin"] and c["index_high_zone"]
    latest["alert"]["volatility_risk"] = bool(alert)
    latest["alert"]["reasons"] = [k for k, v in c.items() if v]

    state["latest"] = latest
    return latest


def main() -> None:
    s = sess()
    state = load_state()

    # 1) è£å®šæ®‹ï¼ˆæœ€æ–°åˆ†ï¼‰
    arb_dt, arb_url = fetch_latest_arbitrage_excel_url(s)
    arb_xls = download_bytes(s, arb_url)
    arb_buy, arb_sell = parse_arbitrage_excel(arb_xls)

    upsert_history(
        state,
        {
            "date": arb_dt.isoformat(),
            "arb_buy": arb_buy,
            "arb_sell": arb_sell,
            "arb_net": arb_buy - arb_sell,
            "prime_volume": None,
            "signals": {},
            "src": {"arb_excel": arb_url},
        },
    )

    # 2) æ—¥å ±ï¼ˆæœ€æ–°åˆ†ï¼‰ãƒ—ãƒ©ã‚¤ãƒ å£²è²·é«˜
    vol_dt, pdf_url = fetch_latest_daily_pdf_url(s)
    pdf_bytes = download_bytes(s, pdf_url)
    prime_volume = extract_prime_volume_from_pdf(pdf_bytes)

    upsert_history(
        state,
        {
            "date": vol_dt.isoformat(),
            "arb_buy": None,
            "arb_sell": None,
            "arb_net": None,
            "prime_volume": prime_volume,
            "signals": {},
            "src": {"daily_pdf": pdf_url},
        },
    )

    # 3) æŒ‡æ•°é«˜å€¤åœ
    index_info = fetch_index_high_zone(INDEX_TICKER, INDEX_PCTL, INDEX_LOOKBACK)

    # 4) åˆ¤å®šã¾ã¨ã‚ï¼ˆlatestï¼‰
    latest = compute_latest(state, index_info)
    save_state(state)

    # 5) runãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ¬ãƒãƒ¼ãƒˆå½¢å¼ï¼‰
    print_report(latest)


if __name__ == "__main__":
    main()
